# Entity Extraction Fine-tuning Project

This project fine-tunes the HuggingFaceTB/SmolLM-360M model for calendar event entity extraction using custom PyTorch training and a specialized loss function. **Achieves 83.2% exact match accuracy** with comprehensive evaluation and production-ready deployment options.

## Overview

The goal is to extract structured entities from natural language calendar event descriptions. The model transforms text like "Meeting with John tomorrow at 2pm for 1 hour" into structured JSON format.

### Extracted Fields
- **action**: Type of event (e.g., "meeting", "lunch")
- **date**: Date of the event (DD/MM/YYYY format)
- **time**: Time of the event (HH:MM AM/PM format)
- **attendees**: List of people attending (or null)
- **location**: Event location (or null)
- **duration**: Duration of the event (or null)
- **recurrence**: Recurrence pattern (or null)
- **notes**: Additional notes (or null)

### Key Results
- **Exact Match Accuracy**: 83.2% (improved from 31.5% baseline)
- **Valid JSON Rate**: 91.6% (improved from 0% baseline)  
- **Field-wise Accuracy**: 85.7% - 97.5% across all entity types

### Example Transformation
```
Input:  "Meeting with John tomorrow at 2pm for 1 hour"
Output: {
  "action": "Meeting",
  "date": "15/01/2024",
  "time": "2:00 PM", 
  "attendees": ["John"],
  "location": null,
  "duration": "1 hour",
  "recurrence": null,
  "notes": null
}
```

## Features

### ğŸ¯ Custom Entity-Aware Loss Function
- Focuses training on entity extraction accuracy
- Weights the output portion more heavily
- Improves structured JSON generation

### ğŸ—ï¸ Custom PyTorch Trainer
- Built from scratch without HuggingFace SFTTrainer
- Comprehensive evaluation metrics
- Real-time training monitoring
- Automatic checkpointing

### ğŸ“Š Comprehensive Evaluation
- Field-wise accuracy metrics
- JSON parsing quality assessment
- Error pattern analysis
- Baseline vs fine-tuned comparison

### ğŸš€ Multiple Deployment Options
- **HuggingFace Hub**: Public model repository with inference API


## Project Structure

```
interview-kickstart/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ test_split.json          # Test dataset (15%)
â”‚   â”‚   â”œâ”€â”€ train_split.json         # Training dataset (70%)
â”‚   â”‚   â””â”€â”€ val_split.json           # Validation dataset (15%)
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ event_text_mapping.jsonl # Original dataset (793 samples)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ custom_trainer.py        # Custom PyTorch trainer
â”‚   â”‚   â”œâ”€â”€ data_analysis.py         # Dataset analysis and preprocessing
â”‚   â”‚   â””â”€â”€ model_setup.py           # Model configuration and custom loss
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â””â”€â”€ huggingface_deployment.py # HuggingFace Hub deployment
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ baseline_evaluation.py   # Baseline model evaluation
â”‚   â”‚   â”œâ”€â”€ evaluate_existing_model.py # Model evaluation utilities
â”‚   â”‚   â””â”€â”€ evaluate_finetuned_f1.py # F1 metrics evaluation
â”‚   â”œâ”€â”€ testing/
â”‚   â”‚   â”œâ”€â”€ f1_metrics_example.py    # F1 metrics examples
â”‚   â”‚   â””â”€â”€ simple_test_model.py     # Simple model testing
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ main_training.py         # Main training pipeline
â”‚       â””â”€â”€ resume_training.py       # Resume training utilities
â”œâ”€â”€ experiments/                     # Training experiment results
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ logs/                        # Training logs
â”‚   â”œâ”€â”€ models/                      # Saved model checkpoints
â”‚   â”œâ”€â”€ plots/                       # Evaluation plots and visualizations
â”‚   â””â”€â”€ results/                     # Evaluation results (JSON)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ TECHNICAL_DOCUMENTATION.md       # Comprehensive technical documentation
```

## Quick Start

### 1. Setup Environment

```bash
# Clone and navigate to project
cd interview-kickstart

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Analysis

```bash
# Analyze the dataset
python src/core/data_analysis.py
```

This will:
- Show dataset statistics (793 samples)
- Display sample examples
- Create train/validation/test splits (70/15/15)
- Save splits to `data/processed/` directory

### 3. Baseline Evaluation

```bash
# Evaluate baseline model performance
python src/evaluation/baseline_evaluation.py
```

### 4. Fine-tuning

```bash
# Run complete training pipeline
python src/training/main_training.py

# Or with custom parameters
python src/training/main_training.py \
    --batch_size 4 \
    --learning_rate 3e-5 \
    --num_epochs 5 \
    --entity_loss_weight 3.0
```

### 5. Deployment (Multiple Options as Required)



#### Option 2: HuggingFace Hub Deployment (Public Access)
```bash
# Set your HuggingFace token
export HF_TOKEN=your_hf_token_here

# Deploy to HuggingFace Hub
python src/deployment/huggingface_deployment.py \
  --model_path outputs/models/best_model.pt \
  --repo_name Shresth12345/entity-extractor

# Use via transformers
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Shresth12345/entity-extraction-smollm")
model = AutoModelForCausalLM.from_pretrained("Shresth12345/entity-extraction-smollm")
```

#### Option 3: Evaluation with F1 Metrics
```bash
# Comprehensive evaluation with F1 scores
python src/evaluation/evaluate_finetuned_f1.py

# Evaluate existing model checkpoint
python src/evaluation/evaluate_existing_model.py
```

#### Option 4: Testing
```bash
# Test F1 metrics implementation
python src/testing/f1_metrics_example.py

# Simple model testing
python src/testing/simple_test_model.py
```

## Detailed Usage

### Training Configuration

The training process supports extensive customization:

```python
config = ModelConfig(
    model_name="HuggingFaceTB/SmolLM-360M",
    max_length=512,
    learning_rate=5e-5,
    weight_decay=0.01,
    batch_size=8,
    gradient_accumulation_steps=4,
    num_epochs=3,
    entity_loss_weight=2.0  # Custom loss weighting
)
```

### Custom Loss Function

The custom loss function combines:
1. Standard causal language modeling loss
2. Entity-focused loss on the output portion
3. Weighted combination for better entity extraction

### Evaluation Metrics

- **Exact Match Accuracy**: Percentage of samples with all entities correct
- **Field-wise Accuracy**: Per-entity accuracy scores  
- **JSON Quality**: Valid JSON generation rate
- **F1 Metrics**: Exact, token-level, and binary F1 scores
- **Response Completeness**: Rate of complete field extraction



## Methodology

### 1. Dataset Preparation
- Comprehensive analysis of 793 samples
- Stratified train/validation/test splits (70/15/15)
- Input formatting: "Extract entities from: {text}"
- Output formatting: JSON with all entity fields

### 2. Model Architecture
- Base: HuggingFaceTB/SmolLM-360M (not instruct version)
- Custom wrapper with entity-aware loss
- Specialized tokenization handling
- Structured output generation

### 3. Training Strategy
- Custom PyTorch trainer implementation
- Entity-weighted loss function
- Gradient accumulation for effective larger batches
- Cosine annealing learning rate schedule
- Early stopping based on validation performance

### 4. Evaluation Framework
- Baseline performance measurement
- Comprehensive error analysis
- Field-specific accuracy tracking
- JSON parsing quality assessment

## Experiment Results

### Dataset Analysis
- Total samples: 793
- Entity field distribution:
  - action: 100% present, 0% null
  - date: 100% present, 0% null
  - time: ~90% present, ~10% null
  - location: ~85% present, ~15% null
  - duration: ~60% present, ~40% null
  - attendees: ~30% present, ~70% null
  - recurrence: ~5% present, ~95% null
  - notes: ~5% present, ~95% null

### Performance Results (Achieved)

| Metric | Baseline | Fine-tuned | Improvement |
|--------|----------|------------|-------------|
| **Exact Match Accuracy** | 31.5% | **83.2%** | **+51.7pp** |
| **Valid JSON Rate** | 0.0% | **91.6%** | **+91.6pp** |
| **Complete Fields Rate** | 0.0% | **91.6%** | **+91.6pp** |

### Field-wise Performance

| Field | Baseline | Fine-tuned | Improvement |
|-------|----------|------------|-------------|
| action | 0.0% | **91.6%** | +91.6pp |
| date | 0.0% | **89.9%** | +89.9pp |
| time | 0.0% | **85.7%** | +85.7pp |
| attendees | 16.0% | **92.4%** | +76.4pp |
| location | 32.0% | **85.7%** | +53.7pp |
| duration | 10.0% | **89.1%** | +79.1pp |
| recurrence | 98.0% | **97.5%** | -0.5pp |
| notes | 96.0% | **95.8%** | -0.2pp |

## Key Design Decisions

### 1. Custom Loss Function
**Rationale**: Standard causal LM loss treats all tokens equally. Our custom loss emphasizes the output (entity) portion, leading to better structured extraction.

### 2. PyTorch Native Training
**Rationale**: Full control over training process, custom metrics, and specialized evaluation without HuggingFace SFTTrainer constraints.

### 3. Entity-Aware Evaluation
**Rationale**: Standard perplexity doesn't reflect entity extraction quality. Our metrics directly measure extraction accuracy.

### 4. Production-Ready Deployment
**Rationale**: Real-world applicability with API, batch processing, and monitoring capabilities.

## Challenges and Solutions

### Challenge 1: JSON Generation Quality
**Solution**: Custom loss function focusing on output structure + entity-aware evaluation metrics.

### Challenge 2: Null Value Handling
**Solution**: Explicit null value training + balanced evaluation considering both present and absent entities.

### Challenge 3: Variable Text Length
**Solution**: Adaptive tokenization + careful padding/truncation strategy.

### Challenge 4: Evaluation Complexity
**Solution**: Multi-level evaluation (exact match, field-wise, JSON quality) + comprehensive error analysis.

## Future Improvements

1. **Data Augmentation**: Generate more diverse training examples
2. **Multi-task Learning**: Joint training on related NLP tasks
3. **Attention Visualization**: Understand model focus areas
4. **Real-time Learning**: Continuous learning from user feedback
5. **Multilingual Support**: Extend to other languages

## File Descriptions

### Core Implementation (`src/core/`)
- `data_analysis.py`: Dataset loading, analysis, and preprocessing
- `model_setup.py`: Model configuration, custom loss function, and base model setup  
- `custom_trainer.py`: PyTorch trainer with custom training loop and evaluation

### Training (`src/training/`)
- `main_training.py`: Complete training pipeline orchestration
- `resume_training.py`: Resume training from checkpoints

### Evaluation (`src/evaluation/`)
- `baseline_evaluation.py`: Comprehensive baseline model evaluation
- `evaluate_existing_model.py`: Evaluate trained model checkpoints
- `evaluate_finetuned_f1.py`: F1 metrics evaluation and analysis

### Deployment (`src/deployment/`)
- `huggingface_deployment.py`: HuggingFace Hub deployment with model cards

### Testing (`src/testing/`)
- `f1_metrics_example.py`: F1 metrics implementation examples
- `simple_test_model.py`: Simple model testing utilities

### Configuration & Documentation
- `requirements.txt`: Python package dependencies
- `README.md`: Project documentation and usage guide
- `TECHNICAL_DOCUMENTATION.md`: Comprehensive technical documentation

## Technical Stack

- **Framework**: PyTorch (native training, no HuggingFace SFTTrainer)
- **Model**: HuggingFaceTB/SmolLM-360M
- **Deployment**: Flask API + CLI interface
- **Evaluation**: Custom metrics + visualization
- **Data**: JSONL format with structured entities

## Reproducibility

All experiments are designed for reproducibility:
- Fixed random seeds
- Comprehensive logging
- Configuration saving
- Deterministic training procedures

## License

This project is for educational and research purposes as part of an interview assignment.

## Documentation

For detailed technical information, please refer to:
- **[TECHNICAL_DOCUMENTATION.md](TECHNICAL_DOCUMENTATION.md)**: Comprehensive technical documentation with design decisions, methodology, and detailed analysis
- **Experiment Results**: Check `outputs/results/` for detailed evaluation results
- **Training Logs**: Available in `outputs/logs/` for training monitoring
- **Visualizations**: Evaluation plots and analysis in `outputs/plots/`

## Contact

For questions about implementation details or results, please refer to the comprehensive technical documentation and experiment logs.
